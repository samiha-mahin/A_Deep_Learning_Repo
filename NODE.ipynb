{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYMspkVZCtlPNTAHVDyO/n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiha-mahin/A_Deep_Learning_Repo/blob/main/NODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**NODE (Neural Oblivious Decision Ensembles)**\n",
        "\n",
        "# What is NODE?\n",
        "\n",
        "**NODE** stands for **Neural Oblivious Decision Ensembles**. It is a machine learning model that combines ideas from decision trees and neural networks to create a powerful, interpretable model for tabular data.\n",
        "\n",
        "NODE was introduced in the paper:\n",
        "\n",
        "> \"Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data\" (Popov et al., 2019)\n",
        "\n",
        "---\n",
        "\n",
        "# Key Concepts Behind NODE\n",
        "\n",
        "### 1. **Oblivious Decision Trees**\n",
        "\n",
        "* An **oblivious decision tree** is a special kind of decision tree where the same feature and threshold are used for all nodes at the same depth.\n",
        "* This means every path down the tree checks features in the *same order*.\n",
        "* This property allows efficient implementation and helps with regularization.\n",
        "\n",
        "### 2. **Neural Networks + Decision Trees**\n",
        "\n",
        "* NODE models decision trees as differentiable functions, which allows training them end-to-end using gradient descent, like neural networks.\n",
        "* Each \"tree\" in NODE is a differentiable module (a soft tree) rather than a hard binary tree.\n",
        "* NODE ensembles multiple such oblivious trees, hence \"ensembles\" in the name.\n",
        "\n",
        "### 3. **Ensemble of Trees**\n",
        "\n",
        "* The final prediction is made by combining outputs of multiple oblivious trees.\n",
        "* The ensemble is trained jointly.\n",
        "\n",
        "### 4. **Advantages of NODE**\n",
        "\n",
        "* Works well on tabular data (which is traditionally challenging for deep nets).\n",
        "* End-to-end differentiable: can be combined with other neural architectures.\n",
        "* More interpretable than typical black-box neural nets.\n",
        "* Can handle heterogeneous features.\n",
        "\n",
        "---\n",
        "\n",
        "# How NODE Works - Step by Step\n",
        "\n",
        "1. **Input Features**: The model receives input features $x = (x_1, x_2, ..., x_d)$.\n",
        "\n",
        "2. **Soft Oblivious Tree Layers**:\n",
        "\n",
        "   * Each tree layer computes soft decisions at each node using feature thresholds.\n",
        "   * Instead of hard splits, NODE uses sigmoid or softmax functions to create *soft* decisions that are differentiable.\n",
        "\n",
        "3. **Tree Output**:\n",
        "\n",
        "   * Each tree produces a vector of outputs (like leaf node values weighted by soft routing probabilities).\n",
        "\n",
        "4. **Ensemble Aggregation**:\n",
        "\n",
        "   * Outputs of all trees are aggregated (e.g., summed) to form the final prediction.\n",
        "\n",
        "5. **Training**:\n",
        "\n",
        "   * The model is trained with backpropagation to minimize the loss (e.g., classification or regression loss).\n",
        "\n",
        "---\n",
        "\n",
        "# Example to Illustrate NODE\n",
        "\n",
        "Suppose you have a dataset with 3 features:\n",
        "\n",
        "| Age | Salary | Education Level |\n",
        "| --- | ------ | --------------- |\n",
        "| 25  | 50000  | Bachelor        |\n",
        "| 40  | 120000 | Master          |\n",
        "| 35  | 80000  | PhD             |\n",
        "\n",
        "You want to classify whether someone will buy a product (Yes/No).\n",
        "\n",
        "### Traditional Decision Tree:\n",
        "\n",
        "* At node 1, check if Age < 30.\n",
        "* At node 2, check if Salary > 60000.\n",
        "* At node 3, check Education Level == Master.\n",
        "* â€¦ and so on.\n",
        "\n",
        "### NODE:\n",
        "\n",
        "* NODE builds several oblivious trees.\n",
        "* For **each tree**, at depth 1, it might \"softly\" check Age with a learned threshold $t_1$, e.g., sigmoid( Age - $t_1$ ).\n",
        "* At depth 2, it might check Salary with a threshold $t_2$, again softly.\n",
        "* All nodes at the same depth use the same feature and threshold.\n",
        "* Because decisions are soft, the model can backpropagate gradients and learn the thresholds $t_1, t_2, ...$ directly.\n",
        "* Multiple such trees are trained simultaneously, combining their outputs.\n",
        "\n",
        "---\n",
        "\n",
        "# Simple Code Example Using PyTorch-like Pseudocode\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SoftObliviousTree(nn.Module):\n",
        "    def __init__(self, num_features, depth):\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        # For each depth level, learn a feature index and a threshold\n",
        "        self.feature_idx = nn.Parameter(torch.randint(0, num_features, (depth,)))\n",
        "        self.thresholds = nn.Parameter(torch.randn(depth))\n",
        "        # Each leaf has a learnable output value\n",
        "        self.leaf_values = nn.Parameter(torch.randn(2**depth))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x shape: batch_size x num_features\n",
        "        batch_size = x.size(0)\n",
        "        decisions = []\n",
        "        \n",
        "        for d in range(self.depth):\n",
        "            f_idx = int(self.feature_idx[d].item())\n",
        "            thresh = self.thresholds[d]\n",
        "            # soft decision: sigmoid of difference\n",
        "            decision = torch.sigmoid(x[:, f_idx] - thresh)\n",
        "            decisions.append(decision)\n",
        "        \n",
        "        # Compute leaf probabilities (soft routing)\n",
        "        leaf_probs = torch.ones(batch_size, 1, device=x.device)\n",
        "        for d in range(self.depth):\n",
        "            leaf_probs = torch.cat([leaf_probs * decisions[d].unsqueeze(1),\n",
        "                                    leaf_probs * (1 - decisions[d]).unsqueeze(1)], dim=1)\n",
        "            leaf_probs = leaf_probs.view(batch_size, -1)\n",
        "        \n",
        "        # Output: weighted sum of leaf values\n",
        "        output = torch.sum(leaf_probs * self.leaf_values, dim=1)\n",
        "        return output\n",
        "\n",
        "class NODEEnsemble(nn.Module):\n",
        "    def __init__(self, num_features, num_trees, depth):\n",
        "        super().__init__()\n",
        "        self.trees = nn.ModuleList([SoftObliviousTree(num_features, depth) for _ in range(num_trees)])\n",
        "    \n",
        "    def forward(self, x):\n",
        "        outputs = [tree(x) for tree in self.trees]\n",
        "        # sum outputs from all trees\n",
        "        return torch.stack(outputs, dim=0).sum(dim=0)\n",
        "\n",
        "# Example usage\n",
        "model = NODEEnsemble(num_features=3, num_trees=5, depth=3)\n",
        "x = torch.tensor([[25, 50000, 1], [40, 120000, 2]], dtype=torch.float32)  # encode education level as numbers\n",
        "output = model(x)\n",
        "print(output)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "| Aspect           | NODE                                                    |\n",
        "| ---------------- | ------------------------------------------------------- |\n",
        "| Model type       | Ensemble of differentiable oblivious trees (soft trees) |\n",
        "| Input            | Tabular data (numerical or categorical)                 |\n",
        "| Training         | End-to-end with gradient descent                        |\n",
        "| Interpretability | More interpretable than typical NNs                     |\n",
        "| Use cases        | Tabular data classification/regression                  |\n",
        "| Key benefit      | Combines tree structure with neural nets                |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "c58tRnTAyYrq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V8d3ZqryUhe"
      },
      "outputs": [],
      "source": []
    }
  ]
}