{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlcfsW89WISnWF2RMK2BRQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiha-mahin/A_Deep_Learning_Repo/blob/main/Introduction_To_Deep_Learnig.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Forward Propagation** and **Backward Propagation**:\n",
        "\n",
        "\n",
        "\n",
        "## üéØ Goal:\n",
        "\n",
        "Let‚Äôs say you built a simple neural network that predicts whether a student will pass or fail based on:\n",
        "\n",
        "* **Hours studied**\n",
        "* **Sleep hours**\n",
        "\n",
        "You want the model to learn how these two inputs affect the output: ‚ÄúPass‚Äù or ‚ÄúFail‚Äù.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Your Neural Network\n",
        "\n",
        "Let‚Äôs say you have this tiny neural network:\n",
        "\n",
        "```\n",
        "Input Layer:         Hidden Layer:        Output Layer:\n",
        "[hours_studied] ---> (Neuron1)          ---> (Prediction)\n",
        "[sleep_hours]    ---> (Neuron2)\n",
        "```\n",
        "\n",
        "You‚Äôll use:\n",
        "\n",
        "* Activation function: Sigmoid (squashes numbers between 0 and 1)\n",
        "* Output close to **1 = Pass**, close to **0 = Fail**\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Forward Propagation (How the network makes a prediction)\n",
        "\n",
        "This is like **guessing the result** based on input.\n",
        "\n",
        "### Suppose:\n",
        "\n",
        "* `hours_studied = 2`\n",
        "* `sleep_hours = 5`\n",
        "\n",
        "The model starts with **random weights** (which it will later adjust):\n",
        "\n",
        "* Weight for hours\\_studied = `0.4`\n",
        "* Weight for sleep\\_hours = `0.3`\n",
        "* Bias = `0.2`\n",
        "\n",
        "### Step 1: Multiply inputs with weights\n",
        "\n",
        "```\n",
        "z = (2 √ó 0.4) + (5 √ó 0.3) + 0.2\n",
        "  = 0.8 + 1.5 + 0.2 = 2.5\n",
        "```\n",
        "\n",
        "### Step 2: Apply activation (Sigmoid function)\n",
        "\n",
        "```\n",
        "Sigmoid(2.5) ‚âà 0.92\n",
        "```\n",
        "\n",
        "So, the model predicts:\n",
        "\n",
        "> ‚ÄúThere‚Äôs a 92% chance the student will **Pass**.‚Äù\n",
        "\n",
        "üéâ That‚Äôs **Forward Propagation** ‚Äî the model uses the input, weights, and bias to make a prediction.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Backward Propagation (How the network **learns**)\n",
        "\n",
        "Now suppose the **actual result** was:\n",
        "\n",
        "> The student **Failed** (i.e., actual = 0)\n",
        "\n",
        "So, your model predicted `0.92` but the correct answer was `0`.\n",
        "It made a big mistake!\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Calculate **Loss**\n",
        "\n",
        "Loss is like the penalty for being wrong.\n",
        "We use something like Mean Squared Error:\n",
        "\n",
        "```\n",
        "Loss = (Predicted - Actual)¬≤\n",
        "     = (0.92 - 0)¬≤ ‚âà 0.85\n",
        "```\n",
        "\n",
        "High loss means \"bad prediction.\"\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Backward Propagation = Fixing the Mistake\n",
        "\n",
        "Here‚Äôs what happens:\n",
        "\n",
        "1. The model figures out **which weights caused the mistake**.\n",
        "2. It adjusts each weight **a little** using gradients (how sensitive the loss is to each weight).\n",
        "3. This is done using something called **Gradient Descent** (a method to reduce the loss).\n",
        "4. Updated weights make the model better next time.\n",
        "\n",
        "### Example: Updating one weight\n",
        "\n",
        "Let‚Äôs say:\n",
        "\n",
        "* The weight for `hours_studied` caused more error than `sleep_hours`\n",
        "* So we **reduce** that weight from `0.4` ‚Üí `0.3`\n",
        "\n",
        "Next time, the prediction will move closer to the actual result.\n",
        "\n",
        "This is **learning**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Repeat\n",
        "\n",
        "Forward ‚Üí Predict\n",
        "Backward ‚Üí Fix mistakes\n",
        "Repeat for many students (many epochs), and the network gets really smart!\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary Table\n",
        "\n",
        "| Step      | Forward Propagation                | Backward Propagation                       |\n",
        "| --------- | ---------------------------------- | ------------------------------------------ |\n",
        "| Purpose   | Make a prediction                  | Learn from the mistake                     |\n",
        "| Data Flow | Input ‚Üí Output                     | Output ‚Üí Error ‚Üí Input (adjust weights)    |\n",
        "| Uses      | Weights, bias, activation function | Loss function, gradients, optimizer        |\n",
        "| Outcome   | A guess                            | Improved weights for better future guesses |\n",
        "\n"
      ],
      "metadata": {
        "id": "Y3iI_aetmv0e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pje1vyOnmGyp"
      },
      "outputs": [],
      "source": []
    }
  ]
}