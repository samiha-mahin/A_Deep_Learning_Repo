{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPE5CPpPWBq7VUG/1LE+vWL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiha-mahin/A_Deep_Learning_Repo/blob/main/Introduction_To_Deep_Learnig.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Forward Propagation** and **Backward Propagation**:\n",
        "\n",
        "\n",
        "\n",
        "## 🎯 Goal:\n",
        "\n",
        "Let’s say you built a simple neural network that predicts whether a student will pass or fail based on:\n",
        "\n",
        "* **Hours studied**\n",
        "* **Sleep hours**\n",
        "\n",
        "You want the model to learn how these two inputs affect the output: “Pass” or “Fail”.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Your Neural Network\n",
        "\n",
        "Let’s say you have this tiny neural network:\n",
        "\n",
        "```\n",
        "Input Layer:         Hidden Layer:        Output Layer:\n",
        "[hours_studied] ---> (Neuron1)          ---> (Prediction)\n",
        "[sleep_hours]    ---> (Neuron2)\n",
        "```\n",
        "\n",
        "You’ll use:\n",
        "\n",
        "* Activation function: Sigmoid (squashes numbers between 0 and 1)\n",
        "* Output close to **1 = Pass**, close to **0 = Fail**\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ Forward Propagation (How the network makes a prediction)\n",
        "\n",
        "This is like **guessing the result** based on input.\n",
        "\n",
        "### Suppose:\n",
        "\n",
        "* `hours_studied = 2`\n",
        "* `sleep_hours = 5`\n",
        "\n",
        "The model starts with **random weights** (which it will later adjust):\n",
        "\n",
        "* Weight for hours\\_studied = `0.4`\n",
        "* Weight for sleep\\_hours = `0.3`\n",
        "* Bias = `0.2`\n",
        "\n",
        "### Step 1: Multiply inputs with weights\n",
        "\n",
        "```\n",
        "z = (2 × 0.4) + (5 × 0.3) + 0.2\n",
        "  = 0.8 + 1.5 + 0.2 = 2.5\n",
        "```\n",
        "\n",
        "### Step 2: Apply activation (Sigmoid function)\n",
        "\n",
        "```\n",
        "Sigmoid(2.5) ≈ 0.92\n",
        "```\n",
        "\n",
        "So, the model predicts:\n",
        "\n",
        "> “There’s a 92% chance the student will **Pass**.”\n",
        "\n",
        "🎉 That’s **Forward Propagation** — the model uses the input, weights, and bias to make a prediction.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 Backward Propagation (How the network **learns**)\n",
        "\n",
        "Now suppose the **actual result** was:\n",
        "\n",
        "> The student **Failed** (i.e., actual = 0)\n",
        "\n",
        "So, your model predicted `0.92` but the correct answer was `0`.\n",
        "It made a big mistake!\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Calculate **Loss**\n",
        "\n",
        "Loss is like the penalty for being wrong.\n",
        "We use something like Mean Squared Error:\n",
        "\n",
        "```\n",
        "Loss = (Predicted - Actual)²\n",
        "     = (0.92 - 0)² ≈ 0.85\n",
        "```\n",
        "\n",
        "High loss means \"bad prediction.\"\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Backward Propagation = Fixing the Mistake\n",
        "\n",
        "Here’s what happens:\n",
        "\n",
        "1. The model figures out **which weights caused the mistake**.\n",
        "2. It adjusts each weight **a little** using gradients (how sensitive the loss is to each weight).\n",
        "3. This is done using something called **Gradient Descent** (a method to reduce the loss).\n",
        "4. Updated weights make the model better next time.\n",
        "\n",
        "### Example: Updating one weight\n",
        "\n",
        "Let’s say:\n",
        "\n",
        "* The weight for `hours_studied` caused more error than `sleep_hours`\n",
        "* So we **reduce** that weight from `0.4` → `0.3`\n",
        "\n",
        "Next time, the prediction will move closer to the actual result.\n",
        "\n",
        "This is **learning**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 Repeat\n",
        "\n",
        "Forward → Predict\n",
        "Backward → Fix mistakes\n",
        "Repeat for many students (many epochs), and the network gets really smart!\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Summary Table\n",
        "\n",
        "| Step      | Forward Propagation                | Backward Propagation                       |\n",
        "| --------- | ---------------------------------- | ------------------------------------------ |\n",
        "| Purpose   | Make a prediction                  | Learn from the mistake                     |\n",
        "| Data Flow | Input → Output                     | Output → Error → Input (adjust weights)    |\n",
        "| Uses      | Weights, bias, activation function | Loss function, gradients, optimizer        |\n",
        "| Outcome   | A guess                            | Improved weights for better future guesses |\n",
        "\n"
      ],
      "metadata": {
        "id": "Y3iI_aetmv0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loss Function**\n",
        "\n",
        "\n",
        "\n",
        "## 💥 What is a Loss Function?\n",
        "\n",
        "A **loss function** tells us **how wrong** the model’s prediction is.\n",
        "\n",
        "Think of it like this:\n",
        "\n",
        "> 🧠 “Hey model, you guessed 92%, but the real answer was 0. That’s way off! Here’s how badly you messed up.”\n",
        "\n",
        "The loss function **quantifies the mistake**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔢 Simple Example\n",
        "\n",
        "Imagine this case:\n",
        "\n",
        "| Input (Hours Studied) | Actual Result | Model’s Prediction |\n",
        "| --------------------- | ------------- | ------------------ |\n",
        "| 2                     | 0 (Fail)      | 0.92               |\n",
        "\n",
        "We use a **loss function** to measure the difference between:\n",
        "\n",
        "* Actual = 0\n",
        "* Predicted = 0.92\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 Common Loss Functions (Simplified)\n",
        "\n",
        "### 1. **Mean Squared Error (MSE)**\n",
        "\n",
        "Used in **regression** (predicting numbers)\n",
        "\n",
        "```text\n",
        "MSE = (1/n) * Σ(predicted - actual)²\n",
        "```\n",
        "\n",
        "#### Example:\n",
        "\n",
        "```\n",
        "Loss = (0.92 - 0)² = 0.8464\n",
        "```\n",
        "\n",
        "So the model’s mistake is 0.8464 — quite high.\n",
        "Lower MSE = better model.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Binary Cross Entropy**\n",
        "\n",
        "Used for **binary classification** (like Pass/Fail, Yes/No)\n",
        "\n",
        "```text\n",
        "Loss = - [ y * log(p) + (1 - y) * log(1 - p) ]\n",
        "```\n",
        "\n",
        "* `y` is actual (0 or 1)\n",
        "* `p` is predicted probability (like 0.92)\n",
        "\n",
        "#### Example:\n",
        "\n",
        "If actual = 0, and predicted = 0.92:\n",
        "\n",
        "```\n",
        "Loss = - [0 * log(0.92) + (1 - 0) * log(1 - 0.92)]\n",
        "     = - log(0.08) ≈ 2.525\n",
        "```\n",
        "\n",
        "🔴 Very high! Because the model was confident about a wrong answer.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Categorical Cross Entropy**\n",
        "\n",
        "Used when there are **more than 2 classes** (e.g., Dog, Cat, Bird)\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 In Simple Words:\n",
        "\n",
        "| Loss Function            | Use For                    | What it Measures                         |\n",
        "| ------------------------ | -------------------------- | ---------------------------------------- |\n",
        "| MSE                      | Regression                 | Squared difference between guess & truth |\n",
        "| Binary CrossEntropy      | Binary classification      | How confident and correct the guess was  |\n",
        "| Categorical CrossEntropy | Multi-class classification | Confidence over many options             |\n",
        "\n",
        "---\n",
        "\n",
        "## 📉 Why It’s Important\n",
        "\n",
        "The model uses the **loss** to:\n",
        "\n",
        "* Know **how bad** its predictions are\n",
        "* Use **backpropagation** to fix its weights\n",
        "* Improve predictions over time\n",
        "\n",
        "> Loss is like a teacher giving a grade — the model learns from it.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IuywbUvGqNoT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pje1vyOnmGyp"
      },
      "outputs": [],
      "source": []
    }
  ]
}