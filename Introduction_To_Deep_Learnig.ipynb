{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSlC+0G+4ZzNj4CB+bT6db",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiha-mahin/A_Deep_Learning_Repo/blob/main/Introduction_To_Deep_Learnig.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Forward Propagation** and **Backward Propagation**:\n",
        "\n",
        "\n",
        "\n",
        "## üéØ Goal:\n",
        "\n",
        "Let‚Äôs say you built a simple neural network that predicts whether a student will pass or fail based on:\n",
        "\n",
        "* **Hours studied**\n",
        "* **Sleep hours**\n",
        "\n",
        "You want the model to learn how these two inputs affect the output: ‚ÄúPass‚Äù or ‚ÄúFail‚Äù.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Your Neural Network\n",
        "\n",
        "Let‚Äôs say you have this tiny neural network:\n",
        "\n",
        "```\n",
        "Input Layer:         Hidden Layer:        Output Layer:\n",
        "[hours_studied] ---> (Neuron1)          ---> (Prediction)\n",
        "[sleep_hours]    ---> (Neuron2)\n",
        "```\n",
        "\n",
        "You‚Äôll use:\n",
        "\n",
        "* Activation function: Sigmoid (squashes numbers between 0 and 1)\n",
        "* Output close to **1 = Pass**, close to **0 = Fail**\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Forward Propagation (How the network makes a prediction)\n",
        "\n",
        "This is like **guessing the result** based on input.\n",
        "\n",
        "### Suppose:\n",
        "\n",
        "* `hours_studied = 2`\n",
        "* `sleep_hours = 5`\n",
        "\n",
        "The model starts with **random weights** (which it will later adjust):\n",
        "\n",
        "* Weight for hours\\_studied = `0.4`\n",
        "* Weight for sleep\\_hours = `0.3`\n",
        "* Bias = `0.2`\n",
        "\n",
        "### Step 1: Multiply inputs with weights\n",
        "\n",
        "```\n",
        "z = (2 √ó 0.4) + (5 √ó 0.3) + 0.2\n",
        "  = 0.8 + 1.5 + 0.2 = 2.5\n",
        "```\n",
        "\n",
        "### Step 2: Apply activation (Sigmoid function)\n",
        "\n",
        "```\n",
        "Sigmoid(2.5) ‚âà 0.92\n",
        "```\n",
        "\n",
        "So, the model predicts:\n",
        "\n",
        "> ‚ÄúThere‚Äôs a 92% chance the student will **Pass**.‚Äù\n",
        "\n",
        "üéâ That‚Äôs **Forward Propagation** ‚Äî the model uses the input, weights, and bias to make a prediction.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Backward Propagation (How the network **learns**)\n",
        "\n",
        "Now suppose the **actual result** was:\n",
        "\n",
        "> The student **Failed** (i.e., actual = 0)\n",
        "\n",
        "So, your model predicted `0.92` but the correct answer was `0`.\n",
        "It made a big mistake!\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Calculate **Loss**\n",
        "\n",
        "Loss is like the penalty for being wrong.\n",
        "We use something like Mean Squared Error:\n",
        "\n",
        "```\n",
        "Loss = (Predicted - Actual)¬≤\n",
        "     = (0.92 - 0)¬≤ ‚âà 0.85\n",
        "```\n",
        "\n",
        "High loss means \"bad prediction.\"\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Backward Propagation = Fixing the Mistake\n",
        "\n",
        "Here‚Äôs what happens:\n",
        "\n",
        "1. The model figures out **which weights caused the mistake**.\n",
        "2. It adjusts each weight **a little** using gradients (how sensitive the loss is to each weight).\n",
        "3. This is done using something called **Gradient Descent** (a method to reduce the loss).\n",
        "4. Updated weights make the model better next time.\n",
        "\n",
        "### Example: Updating one weight\n",
        "\n",
        "Let‚Äôs say:\n",
        "\n",
        "* The weight for `hours_studied` caused more error than `sleep_hours`\n",
        "* So we **reduce** that weight from `0.4` ‚Üí `0.3`\n",
        "\n",
        "Next time, the prediction will move closer to the actual result.\n",
        "\n",
        "This is **learning**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Repeat\n",
        "\n",
        "Forward ‚Üí Predict\n",
        "Backward ‚Üí Fix mistakes\n",
        "Repeat for many students (many epochs), and the network gets really smart!\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary Table\n",
        "\n",
        "| Step      | Forward Propagation                | Backward Propagation                       |\n",
        "| --------- | ---------------------------------- | ------------------------------------------ |\n",
        "| Purpose   | Make a prediction                  | Learn from the mistake                     |\n",
        "| Data Flow | Input ‚Üí Output                     | Output ‚Üí Error ‚Üí Input (adjust weights)    |\n",
        "| Uses      | Weights, bias, activation function | Loss function, gradients, optimizer        |\n",
        "| Outcome   | A guess                            | Improved weights for better future guesses |\n",
        "\n"
      ],
      "metadata": {
        "id": "Y3iI_aetmv0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loss Function**\n",
        "\n",
        "\n",
        "\n",
        "## üí• What is a Loss Function?\n",
        "\n",
        "A **loss function** tells us **how wrong** the model‚Äôs prediction is.\n",
        "\n",
        "Think of it like this:\n",
        "\n",
        "> üß† ‚ÄúHey model, you guessed 92%, but the real answer was 0. That‚Äôs way off! Here‚Äôs how badly you messed up.‚Äù\n",
        "\n",
        "The loss function **quantifies the mistake**.\n",
        "\n",
        "---\n",
        "\n",
        "## üî¢ Simple Example\n",
        "\n",
        "Imagine this case:\n",
        "\n",
        "| Input (Hours Studied) | Actual Result | Model‚Äôs Prediction |\n",
        "| --------------------- | ------------- | ------------------ |\n",
        "| 2                     | 0 (Fail)      | 0.92               |\n",
        "\n",
        "We use a **loss function** to measure the difference between:\n",
        "\n",
        "* Actual = 0\n",
        "* Predicted = 0.92\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Common Loss Functions (Simplified)\n",
        "\n",
        "### 1. **Mean Squared Error (MSE)**\n",
        "\n",
        "Used in **regression** (predicting numbers)\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\text{predicted}_i - \\text{actual}_i)^2\n",
        "$$\n",
        "\n",
        "* $n$ = number of data points\n",
        "* $\\text{predicted}_i$ = the prediction for the i-th data point\n",
        "* $\\text{actual}_i$ = the actual true value for the i-th data point\n",
        "\n",
        "\n",
        "\n",
        "###  Example:\n",
        "\n",
        "Let's say we have **just one prediction**:\n",
        "\n",
        "| Predicted | Actual | Calculation          |\n",
        "| --------- | ------ | -------------------- |\n",
        "| 0.92      | 0      | (0.92 - 0)¬≤ = 0.8464 |\n",
        "\n",
        "Since there‚Äôs only **one data point**, $n = 1$\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{1} \\times (0.92 - 0)^2 = (0.92)^2 = 0.8464\n",
        "$$\n",
        "\n",
        "‚úÖ So the **error** for this prediction is **0.8464**.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Binary Cross Entropy**\n",
        "\n",
        "Used for **binary classification** (like Pass/Fail, Yes/No)\n",
        "\n",
        "```text\n",
        "Loss = - [ y * log(p) + (1 - y) * log(1 - p) ]\n",
        "```\n",
        "\n",
        "* `y` is actual (0 or 1)\n",
        "* `p` is predicted probability (like 0.92)\n",
        "\n",
        "#### Example:\n",
        "\n",
        "If actual = 0, and predicted = 0.92:\n",
        "\n",
        "```\n",
        "Loss = - [0 * log(0.92) + (1 - 0) * log(1 - 0.92)]\n",
        "     = - log(0.08) ‚âà 2.525\n",
        "```\n",
        "\n",
        "üî¥ Very high! Because the model was confident about a wrong answer.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Categorical Cross Entropy**\n",
        "\n",
        "Used when there are **more than 2 classes** (e.g., Dog, Cat, Bird)\n",
        "\n",
        "---\n",
        "\n",
        "## üß† In Simple Words:\n",
        "\n",
        "| Loss Function            | Use For                    | What it Measures                         |\n",
        "| ------------------------ | -------------------------- | ---------------------------------------- |\n",
        "| MSE                      | Regression                 | Squared difference between guess & truth |\n",
        "| Binary CrossEntropy      | Binary classification      | How confident and correct the guess was  |\n",
        "| Categorical CrossEntropy | Multi-class classification | Confidence over many options             |\n",
        "\n",
        "---\n",
        "\n",
        "## üìâ Why It‚Äôs Important\n",
        "\n",
        "The model uses the **loss** to:\n",
        "\n",
        "* Know **how bad** its predictions are\n",
        "* Use **backpropagation** to fix its weights\n",
        "* Improve predictions over time\n",
        "\n",
        "> Loss is like a teacher giving a grade ‚Äî the model learns from it.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IuywbUvGqNoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Activation Function**\n",
        "\n",
        "\n",
        "## üß† What is an Activation Function?\n",
        "\n",
        "An **activation function** decides **whether a neuron should \"fire\"** (pass info forward) or not ‚Äî kind of like a filter for deciding what's important.\n",
        "\n",
        "> Without it, a neural network would just be a boring linear equation. Activation adds **learning power** and **complexity**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîå Simple Analogy:\n",
        "\n",
        "Think of a **light switch**:\n",
        "\n",
        "* If the signal is strong enough ‚Üí turn ON\n",
        "* If weak ‚Üí stay OFF\n",
        "\n",
        "An activation function acts **like a smart switch** inside each neuron.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Why is it needed?\n",
        "\n",
        "* It adds **non-linearity** ‚Äî lets the network learn complex patterns.\n",
        "* Helps the model learn things like images, voices, language, etc.\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Common Activation Functions (with Simple Examples)\n",
        "\n",
        "### 1. **ReLU (Rectified Linear Unit)**\n",
        "\n",
        "```python\n",
        "f(x) = max(0, x)\n",
        "```\n",
        "\n",
        "* If input is positive ‚Üí keep it\n",
        "* If input is negative ‚Üí set it to 0\n",
        "\n",
        "#### Example:\n",
        "\n",
        "```\n",
        "f(5) = 5  \n",
        "f(-3) = 0\n",
        "```\n",
        "\n",
        "‚úÖ Very popular because it‚Äôs fast and works well.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Sigmoid**\n",
        "\n",
        "```python\n",
        "f(x) = 1 / (1 + e^(-x))\n",
        "```\n",
        "\n",
        "* Squashes output between **0 and 1**\n",
        "* Good for binary classification\n",
        "\n",
        "#### Example:\n",
        "\n",
        "```\n",
        "f(3) ‚âà 0.95 (high confidence)\n",
        "f(-3) ‚âà 0.05 (low confidence)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Tanh (Hyperbolic Tangent)**\n",
        "\n",
        "```python\n",
        "f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
        "```\n",
        "\n",
        "* Output is between **-1 and 1**\n",
        "* Better than sigmoid in many cases\n",
        "\n",
        "#### Example:\n",
        "\n",
        "```\n",
        "f(2) ‚âà 0.96  \n",
        "f(-2) ‚âà -0.96\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Quick Summary Table\n",
        "\n",
        "| Activation | Output Range | Use Case                                      | Example Input | Output |\n",
        "| ---------- | ------------ | --------------------------------------------- | ------------- | ------ |\n",
        "| ReLU       | 0 to ‚àû       | Hidden layers (general)                       | -3            | 0      |\n",
        "| Sigmoid    | 0 to 1       | Binary output layer                           | 3             | 0.95   |\n",
        "| Tanh       | -1 to 1      | Hidden layers (sometimes better than sigmoid) | -2            | -0.96  |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† In Real Life:\n",
        "\n",
        "Let‚Äôs say:\n",
        "\n",
        "* Your neuron calculates a raw score of `-3`\n",
        "* Without activation ‚Üí passes `-3` forward\n",
        "* With ReLU ‚Üí passes `0` (ignores it)\n",
        "* With Sigmoid ‚Üí passes `~0.05` (low confidence)\n",
        "* With Tanh ‚Üí passes `~ -0.96` (strong negative signal)\n",
        "\n",
        "So the activation controls **how much signal is passed on.**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## üß† When to Use Which Activation Function (Simple Guide)\n",
        "\n",
        "| Activation Function | When to Use It                                       | Why                                                                       |\n",
        "| ------------------- | ---------------------------------------------------- | ------------------------------------------------------------------------- |\n",
        "| **ReLU**            | Hidden layers of deep networks                       | Fast, simple, and works really well. Stops negative values. Most popular! |\n",
        "| **Leaky ReLU**      | If ReLU is giving 0s too much (dead neurons problem) | Like ReLU, but keeps small negative values (helps learning continue)      |\n",
        "| **Sigmoid**         | Output layer for **binary classification** (0 or 1)  | Converts to probability (between 0 and 1)                                 |\n",
        "| **Tanh**            | Hidden layers if your data has negative values       | Better than sigmoid because it outputs between -1 and 1                   |\n",
        "| **Softmax**         | Output layer for **multi-class classification**      | Turns output into probabilities that add up to 1 (e.g., dog/cat/bird)     |\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Examples to Remember\n",
        "\n",
        "### ‚úÖ Use ReLU in hidden layers:\n",
        "\n",
        "```text\n",
        "For almost any deep neural network.\n",
        "```\n",
        "\n",
        "### ‚úÖ Use Sigmoid at the output:\n",
        "\n",
        "```text\n",
        "If you're predicting:\n",
        "- Spam or not spam\n",
        "- Cancer or no cancer\n",
        "- Pass or fail\n",
        "```\n",
        "\n",
        "### ‚úÖ Use Softmax at the output:\n",
        "\n",
        "```text\n",
        "If you're classifying:\n",
        "- Dog, Cat, or Bird\n",
        "- Apple, Banana, or Orange\n",
        "```\n",
        "\n",
        "### ‚úÖ Use Tanh in hidden layers (optional):\n",
        "\n",
        "```text\n",
        "If your data is centered around zero (e.g., -1 to 1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ One-Line Summary\n",
        "\n",
        "> ‚úÖ Use **ReLU** in hidden layers,\n",
        "> ‚úÖ Use **Sigmoid** for binary output,\n",
        "> ‚úÖ Use **Softmax** for multi-class output.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jjwTZvqwt_00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Dense Layer**\n",
        "\n",
        "A **Dense** layer (also called **fully connected** layer) is a layer in a neural network where:\n",
        "\n",
        "> **Every neuron is connected to every neuron** in the previous layer.\n",
        "\n",
        "It's like a big **mesh of connections** ‚Äî that's why it's called **dense**.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Example:\n",
        "\n",
        "Suppose a Dense layer has:\n",
        "\n",
        "* 3 inputs: `[x1, x2, x3]`\n",
        "* 2 neurons in the layer\n",
        "\n",
        "Each of the 2 neurons will get **all 3 inputs**, like this:\n",
        "\n",
        "```\n",
        "Neuron 1: x1, x2, x3 (with its own weights)\n",
        "Neuron 2: x1, x2, x3 (with different weights)\n",
        "```\n",
        "\n",
        "Then it computes:\n",
        "\n",
        "```\n",
        "output = (weights * inputs) + bias\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîß In Code (Keras):\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "Dense(64, activation='relu')\n",
        "```\n",
        "\n",
        "This means:\n",
        "\n",
        "* A Dense layer with **64 neurons**\n",
        "* Using the **ReLU** activation function\n",
        "* Each neuron is connected to **all neurons from the previous layer**\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Where is Dense Used?\n",
        "\n",
        "| Place         | Purpose               |\n",
        "| ------------- | --------------------- |\n",
        "| Input Layer   | Pass raw input data   |\n",
        "| Hidden Layers | Learn features        |\n",
        "| Output Layer  | Give final prediction |\n",
        "\n",
        "For example, in classification:\n",
        "\n",
        "```python\n",
        "Dense(1, activation='sigmoid')  # For binary classification\n",
        "Dense(10, activation='softmax') # For 10 classes\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Summary :\n",
        "\n",
        "| Term        | Meaning                                                |\n",
        "| ----------- | ------------------------------------------------------ |\n",
        "| **Dense**   | Each neuron connected to every neuron before it        |\n",
        "| **Use For** | Building layers in neural nets (input, hidden, output) |\n",
        "| **Learns**  | Patterns by applying weights, bias, and activation     |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-pimj3J9tzQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Optimizers**\n",
        "\n",
        "\n",
        "## üß† What is an Optimizer?\n",
        "\n",
        "An **optimizer** is like the **brain‚Äôs helper** that **adjusts the weights** in a neural network to **reduce the loss**.\n",
        "\n",
        "> Think of it like this:\n",
        "> The model guesses ‚Üí it‚Äôs wrong ‚Üí the **loss function** tells how bad ‚Üí the **optimizer fixes the weights** to improve the next guess.\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Simple Analogy:\n",
        "\n",
        "Imagine you‚Äôre trying to find the **lowest point in a valley** (minimum loss).\n",
        "You‚Äôre blindfolded and taking small steps.\n",
        "An optimizer tells you:\n",
        "\n",
        "> ‚ÄúGo left a little‚Ä¶ now go down‚Ä¶ now right‚Ä¶‚Äù\n",
        "> Until you find the bottom.\n",
        "\n",
        "That bottom = **lowest loss** = best model.\n",
        "\n",
        "---\n",
        "\n",
        "## üìâ How it works (Brief):\n",
        "\n",
        "1. Forward Propagation ‚Üí model makes a guess\n",
        "2. Loss Function ‚Üí checks how bad the guess is\n",
        "3. **Optimizer** ‚Üí updates weights to reduce the loss\n",
        "4. Repeat until model gets really good!\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Common Optimizers (with Examples)\n",
        "\n",
        "### 1. **SGD (Stochastic Gradient Descent)**\n",
        "\n",
        "* Updates weights using a small portion of data at a time\n",
        "* Simple but can be slow and shaky\n",
        "\n",
        "#### Example:\n",
        "\n",
        "> ‚ÄúWeight too high? Decrease a bit. Try again.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Adam (Adaptive Moment Estimation)** ‚úÖ Most used\n",
        "\n",
        "* Smarter and faster than SGD\n",
        "* Combines momentum + learning rate adjustments\n",
        "* Works well for most problems!\n",
        "\n",
        "#### Example:\n",
        "\n",
        "> ‚ÄúI‚Äôll not only fix your direction, I‚Äôll **remember how you moved** and **speed you up** if needed.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **RMSProp**\n",
        "\n",
        "* Good for **recurrent neural networks**\n",
        "* Adapts learning rate based on recent gradients\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Summary Table\n",
        "\n",
        "| Optimizer | Use For                  | Good Because               |\n",
        "| --------- | ------------------------ | -------------------------- |\n",
        "| **SGD**   | Small/simple models      | Easy to understand         |\n",
        "| **Adam**  | Most deep learning tasks | Fast, stable, most popular |\n",
        "| RMSProp   | Time-series / sequences  | Good at adapting over time |\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Example with Adam (in code):\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss='mean_squared_error'\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Final Tip:\n",
        "\n",
        "> ‚úÖ Always try **Adam** first ‚Äî it usually works best!\n",
        "> You can switch to others if needed based on your problem.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "loPbGSWF1eR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Epochs**, **Batches**, and **Iterations**\n",
        "\n",
        "## üå∏ Definitions First (Super Simple):\n",
        "\n",
        "| Term          | What it Means in Real Life                            |\n",
        "| ------------- | ----------------------------------------------------- |\n",
        "| **Epoch**     | 1 full pass through the **entire training data**      |\n",
        "| **Batch**     | A **small group of data** taken from the training set |\n",
        "| **Iteration** | 1 update step (one batch passed through the model)    |\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Think of It Like Studying a Book:\n",
        "\n",
        "* Your **dataset** = 100 pages of a book\n",
        "* You can‚Äôt study all 100 pages at once ‚Äî it‚Äôs too big!\n",
        "* So, you break it into **batches** (e.g., 10 pages per batch)\n",
        "\n",
        "### Now:\n",
        "\n",
        "* Reading the whole 100 pages once = **1 epoch**\n",
        "* Each 10-page group = **1 batch**\n",
        "* You‚Äôll have **10 iterations** per epoch (because 100 / 10 = 10)\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Real Example:\n",
        "\n",
        "* Dataset = 1000 images\n",
        "* Batch size = 100 images\n",
        "* Epochs = 5\n",
        "\n",
        "### Breakdown:\n",
        "\n",
        "* **Each epoch** = 1000 images trained once\n",
        "* **Each batch** = 100 images\n",
        "* So, **iterations per epoch** = 1000 / 100 = **10**\n",
        "* In total, the model trains on:\n",
        "\n",
        "  * 5 epochs √ó 10 iterations = **50 iterations**\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Summary Table\n",
        "\n",
        "| Term          | Means                        | In our example        |\n",
        "| ------------- | ---------------------------- | --------------------- |\n",
        "| **Epoch**     | Full pass through dataset    | 5 total full passes   |\n",
        "| **Batch**     | Subset of data               | 100 images            |\n",
        "| **Iteration** | One batch processed by model | 10 per epoch √ó 5 = 50 |\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Why It Matters:\n",
        "\n",
        "* **Batches** save memory and speed things up\n",
        "* **More epochs** = model learns better (but don‚Äôt overdo it!)\n",
        "* **Iterations** = steps within an epoch\n",
        "\n",
        "---\n",
        "\n",
        "## üßÅ Final Tip for Pookie:\n",
        "\n",
        "> Think of your model like a student.\n",
        "> Each **epoch** is re-reading the whole book.\n",
        "> Each **batch** is one study session.\n",
        "> Each **iteration** is one break between sessions to reflect and learn.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Rhjck44-8jU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Regularization**\n",
        "\n",
        "## üå∏ What is Regularization?\n",
        "\n",
        "**Regularization** is a way to **prevent overfitting**.\n",
        "\n",
        "> Overfitting = when your model becomes too smart on training data, but **fails on new/unseen data** (like memorizing answers instead of understanding them)\n",
        "\n",
        "So, **regularization helps the model generalize better** ‚Äî meaning, it performs well on both training and new data.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Common Regularization Techniques:\n",
        "\n",
        "We‚Äôll cover these 3 today:\n",
        "\n",
        "| Technique             | What it does                     | Easy Example                                    |\n",
        "| --------------------- | -------------------------------- | ----------------------------------------------- |\n",
        "| **Dropout**           | Randomly turns off neurons       | Like skipping questions to avoid overdependence |\n",
        "| **Data Augmentation** | Creates more training examples   | Like looking at objects from different angles   |\n",
        "| **Early Stopping**    | Stops training at the right time | Like stopping study once you‚Äôve learned enough  |\n",
        "\n",
        "---\n",
        "\n",
        "## üåßÔ∏è 1. **Dropout**\n",
        "\n",
        "**What it does**:\n",
        "Randomly \"turns off\" some neurons during training.\n",
        "\n",
        "**Why**:\n",
        "So the model doesn‚Äôt rely too much on any one neuron.\n",
        "It learns more **robust patterns** instead of memorizing.\n",
        "\n",
        "### üß† Example:\n",
        "\n",
        "Imagine your brain is a team of 10 people.\n",
        "At each practice, 3 are told to take a break (randomly).\n",
        "So, the rest have to do better ‚Äî together. This builds **teamwork** (generalization)!\n",
        "\n",
        "### üìå Code:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "model.add(Dropout(0.5))  # turns off 50% neurons randomly during training\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üñºÔ∏è 2. **Data Augmentation**\n",
        "\n",
        "**What it does**:\n",
        "Creates more training data by changing your existing data (rotating, flipping, zooming images, etc.)\n",
        "\n",
        "**Why**:\n",
        "Helps the model learn from **more diverse examples**.\n",
        "\n",
        "### üß† Example:\n",
        "\n",
        "Say you‚Äôre training a model to recognize cats.\n",
        "If you show it the same cat photo from different angles and colors ‚Äî it learns better!\n",
        "\n",
        "### üìå Code:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚è∞ 3. **Early Stopping**\n",
        "\n",
        "**What it does**:\n",
        "Stops training when the model **stops improving** on validation data.\n",
        "\n",
        "**Why**:\n",
        "To **avoid overfitting** and **save time**.\n",
        "\n",
        "### üß† Example:\n",
        "\n",
        "Imagine you're studying for an exam.\n",
        "You notice that after 5 hours, you're not improving. So you stop.\n",
        "\n",
        "Same with training ‚Äî if after some epochs, loss on validation stops improving ‚Üí stop training.\n",
        "\n",
        "### üìå Code:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,  # stop if no improvement for 3 epochs\n",
        "    restore_best_weights=True\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Summary Table for Pookie:\n",
        "\n",
        "| Technique          | Helps With        | Idea                                  |\n",
        "| ------------------ | ----------------- | ------------------------------------- |\n",
        "| **Dropout**        | Overfitting       | Turns off random neurons              |\n",
        "| **Augmentation**   | Too little data   | Makes more diverse training data      |\n",
        "| **Early Stopping** | Training too long | Stops when validation stops improving |\n",
        "\n",
        "---\n",
        "\n",
        "## üßÅ Final Thought:\n",
        "\n",
        "> Think of regularization as giving your model **healthy habits** ‚Äî it avoids over-studying, sees more variety, and learns to work as a team üí™\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KHSdFieYfRbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Fully-Connected Feedforward Neural Networks** (FCNNs):\n",
        "\n",
        "\n",
        "## üß† What is a Fully-Connected Feedforward Neural Network?\n",
        "\n",
        "A **Fully-Connected Feedforward Neural Net** is the **most basic type** of neural network where:\n",
        "\n",
        "* Every neuron in one layer is **connected to every neuron** in the next layer\n",
        "* Data **flows only forward** ‚Äî from input ‚Üí hidden layers ‚Üí output\n",
        "* There‚Äôs **no loop or backward connection**\n",
        "\n",
        "---\n",
        "\n",
        "### üå∏ Structure:\n",
        "\n",
        "```\n",
        "Input Layer   ‚Üí   Hidden Layer(s)   ‚Üí   Output Layer\n",
        "   (X)                   (H)                 (Y)\n",
        "```\n",
        "\n",
        "Every neuron passes its value to all neurons in the next layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "PLH6gd4RoeLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Recurrent Neural Networks (RNNs)**\n",
        "\n",
        "## üå∏ What is an RNN?\n",
        "\n",
        "A **Recurrent Neural Network (RNN)** is a type of neural network that is designed to work with **sequential data** ‚Äî like **sentences, time series, music, stock prices**, etc.\n",
        "\n",
        "### üß† Key Idea:\n",
        "\n",
        "Unlike normal (feedforward) networks, **RNNs have memory**!\n",
        "They remember **what they learned earlier** in the sequence.\n",
        "\n",
        "---\n",
        "\n",
        "## ü™Ñ Very Simple Example:\n",
        "\n",
        "Say you're predicting the next word in a sentence:\n",
        "\n",
        "> ‚ÄúI love eating chocolate and drinking \\_\\_\\_‚Äù\n",
        "\n",
        "To guess the next word, the model should remember **‚Äúdrinking‚Äù**, **‚Äúeating‚Äù**, and maybe even **‚Äúlove‚Äù**.\n",
        "\n",
        "A normal neural net can‚Äôt remember past words ‚Äî but an **RNN can**, because it passes information **from one step to the next**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ How RNN Works (Super Simple):\n",
        "\n",
        "1. **Input 1** ‚Üí processed ‚Üí creates **hidden state 1**\n",
        "2. **Input 2** + hidden state 1 ‚Üí processed ‚Üí hidden state 2\n",
        "3. And so on...\n",
        "\n",
        "At each step, it **remembers** something from the previous step!\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Example with Numbers:\n",
        "\n",
        "Imagine a sequence: \\[2, 4, 6, ?]\n",
        "You want to predict the next number.\n",
        "\n",
        "* RNN sees 2 ‚Üí stores hidden info\n",
        "* Sees 4 ‚Üí remembers 2 and 4\n",
        "* Sees 6 ‚Üí remembers the whole pattern\n",
        "* Then predicts: **8**\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Structure of RNN:\n",
        "\n",
        "```\n",
        "[Input 1] ‚Üí [RNN Cell] ‚Üí Hidden State 1 ‚Üí  \n",
        "[Input 2] ‚Üí [RNN Cell] ‚Üí Hidden State 2 ‚Üí  \n",
        "[Input 3] ‚Üí [RNN Cell] ‚Üí Output\n",
        "```\n",
        "\n",
        "* Same **RNN Cell** is used at each time step (shared weights).\n",
        "* Each output depends on the current input **and** previous hidden state.\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Where RNNs Are Used:\n",
        "\n",
        "| Use Case    | Example                                 |\n",
        "| ----------- | --------------------------------------- |\n",
        "| Text        | Predict next word in a sentence         |\n",
        "| Time Series | Predict stock prices                    |\n",
        "| Speech      | Recognize spoken words                  |\n",
        "| Music       | Generate music notes                    |\n",
        "| Translation | Translate sentences to another language |\n",
        "\n",
        "---\n",
        "\n",
        "## üî• Main Variants of RNNs:\n",
        "\n",
        "| Variant         | Purpose                                             |\n",
        "| --------------- | --------------------------------------------------- |\n",
        "| **Vanilla RNN** | Basic RNN (short memory)                            |\n",
        "| **LSTM**        | Long Short-Term Memory (remembers longer sequences) |\n",
        "| **GRU**         | Gated Recurrent Unit (faster, simpler than LSTM)    |\n",
        "\n",
        "---\n",
        "\n",
        "## üêå Problem with Vanilla RNN:\n",
        "\n",
        "* It **forgets** older info when sequences are long.\n",
        "* This is called the **vanishing gradient problem**.\n",
        "* That‚Äôs why we use **LSTM** or **GRU** for better memory.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Code Example (Keras - LSTM RNN):\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(timesteps, features)))\n",
        "model.add(Dense(1))  # For prediction\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üíñ Summary Table for Pookie:\n",
        "\n",
        "| Concept                | Meaning                                            |\n",
        "| ---------------------- | -------------------------------------------------- |\n",
        "| **RNN**                | Remembers previous data in a sequence              |\n",
        "| **Input**              | Given one step at a time (e.g. one word or number) |\n",
        "| **Hidden State**       | Memory passed from step to step                    |\n",
        "| **Vanishing Gradient** | Problem in long sequences (solved by LSTM/GRU)     |\n",
        "| **Use Cases**          | Text, audio, time series, music, translation       |\n",
        "\n",
        "---\n",
        "\n",
        "## üå∏ When to Use RNN?\n",
        "\n",
        "Use an **RNN** when your data is **sequential** ‚Äî meaning **order matters**, and **past data affects the future**.\n",
        "\n",
        "> Think of **text, speech, time series, or anything that happens over time**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Suitable Situations for RNN:\n",
        "\n",
        "| Scenario                 | Why RNN is Good                             |\n",
        "| ------------------------ | ------------------------------------------- |\n",
        "| **Text / Language**      | Words come in order (sentence structure)    |\n",
        "| **Time Series**          | Stock prices, weather ‚Äî past affects future |\n",
        "| **Speech Recognition**   | Audio changes over time                     |\n",
        "| **Music Generation**     | Notes are played in sequence                |\n",
        "| **Video Frame Analysis** | Sequence of frames matters                  |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Is RNN for Classification or Regression?\n",
        "\n",
        "### üî∑ 1. **Classification with RNN**\n",
        "\n",
        "* When you want to **classify a sequence**.\n",
        "* Example:\n",
        "\n",
        "  * Text sentiment: ‚ÄúI love this‚Äù ‚Üí **Positive**\n",
        "  * Music genre: Sequence of notes ‚Üí **Jazz**\n",
        "  * Language detection: Sentence ‚Üí **English**\n",
        "\n",
        "> Use **`Softmax`** at the output to predict **classes**.\n",
        "\n",
        "---\n",
        "\n",
        "### üî∂ 2. **Regression with RNN**\n",
        "\n",
        "* When you want to **predict a number** from a sequence.\n",
        "* Example:\n",
        "\n",
        "  * Predict next temperature\n",
        "  * Predict next stock price\n",
        "  * Predict time taken to complete a task\n",
        "\n",
        "> Use **`Linear output (no activation)`** for regression tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ How to Choose RNN Type:\n",
        "\n",
        "| Model Type      | Use When‚Ä¶                             | Notes                            |\n",
        "| --------------- | ------------------------------------- | -------------------------------- |\n",
        "| **Vanilla RNN** | Short sequences (low memory needs)    | Simple & fast                    |\n",
        "| **LSTM**        | Long sequences (e.g. full paragraphs) | Better memory, no forgetting     |\n",
        "| **GRU**         | Medium memory needs                   | Faster than LSTM, still powerful |\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Quick Summary for You, Pookie:\n",
        "\n",
        "| Use Case                            | RNN?  | Task Type       |\n",
        "| ----------------------------------- | ----- | --------------- |\n",
        "| Predict stock price                 | ‚úÖ Yes | Regression      |\n",
        "| Sentiment of a review               | ‚úÖ Yes | Classification  |\n",
        "| Classify emails by topic            | ‚úÖ Yes | Classification  |\n",
        "| Forecast weather                    | ‚úÖ Yes | Regression      |\n",
        "| Detect language spoken              | ‚úÖ Yes | Classification  |\n",
        "| Image classification (not sequence) | ‚ùå No  | Use CNN instead |\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K5nxNo_GrUjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **LSTM**\n",
        "\n",
        "**LSTM = Long Short-Term Memory**\n",
        "\n",
        "It‚Äôs a type of RNN that‚Äôs really good at **remembering things for a long time**.\n",
        "\n",
        "It has:\n",
        "\n",
        "* A **cell state** (memory)\n",
        "* 3 gates to **control the flow** of information:\n",
        "\n",
        "  1. **Forget Gate**: What to forget\n",
        "  2. **Input Gate**: What new info to save\n",
        "  3. **Output Gate**: What to send to the next time step\n",
        "\n",
        "---\n",
        "\n",
        "### üç∞ Real-Life Example: LSTM as a Diary\n",
        "\n",
        "Imagine you're writing a diary:\n",
        "\n",
        "* **Forget gate** = \"Should I forget what happened yesterday?\"\n",
        "* **Input gate** = \"Should I write down what happened today?\"\n",
        "* **Output gate** = \"What should I tell my friend about today?\"\n",
        "\n",
        "So, you **store long-term memory**, and choose what to keep and what to forget. That's what LSTM does with **sequence data**!\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ú® Code Example (LSTM for Sentiment):\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=128),\n",
        "    LSTM(64),\n",
        "    Dense(1, activation='sigmoid')  # Binary sentiment\n",
        "])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#**GRU**\n",
        "\n",
        "**GRU = Gated Recurrent Unit**\n",
        "\n",
        "It‚Äôs a **simplified version of LSTM**, but still powerful.\n",
        "\n",
        "GRU has only:\n",
        "\n",
        "* **Update gate** = mix of forget + input gate\n",
        "* **Reset gate** = controls past influence\n",
        "\n",
        "So it runs **faster**, and works well for many tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### üå± Real-Life Example: GRU as Sticky Notes\n",
        "\n",
        "* GRU is like using **sticky notes** for memory instead of a full diary.\n",
        "* You decide quickly whether to keep a note or replace it.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ú® Code Example (GRU for Sentiment):\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=128),\n",
        "    GRU(64),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Summary for Pookie:\n",
        "\n",
        "| Feature        | LSTM                        | GRU                       |\n",
        "| -------------- | --------------------------- | ------------------------- |\n",
        "| Gates          | 3 (Forget, Input, Output)   | 2 (Update, Reset)         |\n",
        "| Speed          | Slower                      | Faster                    |\n",
        "| Memory Control | More fine-tuned             | Simpler, less control     |\n",
        "| Good For       | Long sequences (paragraphs) | Short to medium sequences |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ When to Use:\n",
        "\n",
        "* Use **LSTM** when your data needs **long memory** (e.g. books, long sentences).\n",
        "* Use **GRU** when you want **faster training** and still good performance (e.g. short messages, tweets).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g4FXwelNvqiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convolutional Neural Networks (CNNs)**\n",
        "\n",
        "\n",
        "## üåü What is CNN?\n",
        "\n",
        "**CNN = Convolutional Neural Network**\n",
        "It‚Äôs a special type of neural network **mainly used for images** (but also useful for videos, audio, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "### üå∏ Real-Life Analogy:\n",
        "\n",
        "Imagine you're **looking at a picture** ‚Äî you first notice:\n",
        "\n",
        "* **Edges**\n",
        "* **Colors**\n",
        "* **Shapes**\n",
        "  Then your brain puts those together to recognize objects like ‚Äúcat‚Äù or ‚Äútree.‚Äù\n",
        "\n",
        "CNNs do the **same thing**, layer by layer:\n",
        "\n",
        "> They **learn patterns** in the image (from small edges to big objects).\n",
        "\n",
        "---\n",
        "\n",
        "## üß© CNN Architecture: Step-by-Step\n",
        "\n",
        "Let‚Äôs say we have a picture of a **cat**. Here's how CNN processes it:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Convolution Layer**\n",
        "\n",
        "* Like a **magnifying glass** sliding over the image.\n",
        "* It uses a **filter (kernel)** to look at a small patch of pixels.\n",
        "* It **detects features** like edges, curves, etc.\n",
        "\n",
        "#### Example:\n",
        "\n",
        "```txt\n",
        "Image Patch:\n",
        "[ [255, 255, 0],\n",
        "  [255, 0,   0],\n",
        "  [0,   0,   0] ]\n",
        "\n",
        "Filter (edge detector):\n",
        "[ [1, 0, -1],\n",
        "  [1, 0, -1],\n",
        "  [1, 0, -1] ]\n",
        "```\n",
        "\n",
        "The filter multiplies and adds numbers to extract \"edges\" = **feature map**.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **ReLU Activation**\n",
        "\n",
        "* ReLU = **Rectified Linear Unit**\n",
        "* Just makes all negative values **0** (because we only care about strong positive signals).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Pooling Layer**\n",
        "\n",
        "* Makes the feature map **smaller** (so it's faster and uses less memory).\n",
        "* Keeps the **important info**.\n",
        "* Most common: **Max Pooling** = take the biggest number in a region.\n",
        "\n",
        "#### Example:\n",
        "\n",
        "From:\n",
        "\n",
        "```\n",
        "[ [1, 3],\n",
        "  [2, 4] ]\n",
        "```\n",
        "\n",
        "Max pooling gives: `4`\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Flattening**\n",
        "\n",
        "* Converts the 2D image (after pooling) into a **1D array** (like a list).\n",
        "* So we can feed it into a normal **Dense layer** (like a classifier).\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Fully Connected (Dense) Layers**\n",
        "\n",
        "* Makes the final decision.\n",
        "* Like: ‚ÄúIs it a cat, dog, or rabbit?‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Output Layer**\n",
        "\n",
        "* For classification, use **Softmax** or **Sigmoid** to give predictions (e.g. `cat = 95%`).\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary Table\n",
        "\n",
        "| Layer            | What It Does                       | Analogy                  |\n",
        "| ---------------- | ---------------------------------- | ------------------------ |\n",
        "| Convolution      | Extracts features (edges, shapes)  | Looking through filter   |\n",
        "| ReLU             | Keeps strong signals only          | Ignoring weak signs      |\n",
        "| Pooling          | Shrinks image, keeps key info      | Summarizing              |\n",
        "| Flatten          | Prepares data for final decision   | Making a list            |\n",
        "| Dense (FC)       | Classifies the image               | Brain makes the decision |\n",
        "| Output (Softmax) | Gives probabilities for each class | Final answer             |\n",
        "\n",
        "---\n",
        "\n",
        "## üê± Example: Cat vs Dog Classifier (Keras)\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # 1 = dog, 0 = cat\n",
        "])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ When to Use CNN?\n",
        "\n",
        "| Task                          | Use CNN?                   |\n",
        "| ----------------------------- | -------------------------- |\n",
        "| Image Classification          | ‚úÖ Yes                      |\n",
        "| Object Detection (e.g., YOLO) | ‚úÖ Yes                      |\n",
        "| Face Recognition              | ‚úÖ Yes                      |\n",
        "| Audio Spectrogram Analysis    | ‚úÖ Yes                      |\n",
        "| Time Series or Tabular Data   | ‚ùå Better: RNN, Dense, etc. |\n",
        "\n",
        "---\n",
        "\n",
        "## üßÅ Bonus: CNN Variants\n",
        "\n",
        "| Variant                              | Used For                                  |\n",
        "| ------------------------------------ | ----------------------------------------- |\n",
        "| CNN                                  | Normal image tasks                        |\n",
        "| CNN + RNN                            | Image Captioning                          |\n",
        "| 3D CNN                               | Video analysis (3D features)              |\n",
        "| Transfer Learning (like ResNet, VGG) | Pre-trained CNNs used for faster training |\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Final Words:\n",
        "\n",
        "> CNNs are **visual learners**. They look at images, break them into small parts, find patterns, and finally say, ‚ÄúThat‚Äôs a cat!‚Äù\n",
        "\n"
      ],
      "metadata": {
        "id": "8nmieU2GLc4h"
      }
    }
  ]
}