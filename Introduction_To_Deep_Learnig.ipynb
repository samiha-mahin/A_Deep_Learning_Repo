{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxS5lJ1fi0qqPXhyrWXwnW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiha-mahin/A_Deep_Learning_Repo/blob/main/Introduction_To_Deep_Learnig.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Forward Propagation** and **Backward Propagation**:\n",
        "\n",
        "\n",
        "\n",
        "## üéØ Goal:\n",
        "\n",
        "Let‚Äôs say you built a simple neural network that predicts whether a student will pass or fail based on:\n",
        "\n",
        "* **Hours studied**\n",
        "* **Sleep hours**\n",
        "\n",
        "You want the model to learn how these two inputs affect the output: ‚ÄúPass‚Äù or ‚ÄúFail‚Äù.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Your Neural Network\n",
        "\n",
        "Let‚Äôs say you have this tiny neural network:\n",
        "\n",
        "```\n",
        "Input Layer:         Hidden Layer:        Output Layer:\n",
        "[hours_studied] ---> (Neuron1)          ---> (Prediction)\n",
        "[sleep_hours]    ---> (Neuron2)\n",
        "```\n",
        "\n",
        "You‚Äôll use:\n",
        "\n",
        "* Activation function: Sigmoid (squashes numbers between 0 and 1)\n",
        "* Output close to **1 = Pass**, close to **0 = Fail**\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Forward Propagation (How the network makes a prediction)\n",
        "\n",
        "This is like **guessing the result** based on input.\n",
        "\n",
        "### Suppose:\n",
        "\n",
        "* `hours_studied = 2`\n",
        "* `sleep_hours = 5`\n",
        "\n",
        "The model starts with **random weights** (which it will later adjust):\n",
        "\n",
        "* Weight for hours\\_studied = `0.4`\n",
        "* Weight for sleep\\_hours = `0.3`\n",
        "* Bias = `0.2`\n",
        "\n",
        "### Step 1: Multiply inputs with weights\n",
        "\n",
        "```\n",
        "z = (2 √ó 0.4) + (5 √ó 0.3) + 0.2\n",
        "  = 0.8 + 1.5 + 0.2 = 2.5\n",
        "```\n",
        "\n",
        "### Step 2: Apply activation (Sigmoid function)\n",
        "\n",
        "```\n",
        "Sigmoid(2.5) ‚âà 0.92\n",
        "```\n",
        "\n",
        "So, the model predicts:\n",
        "\n",
        "> ‚ÄúThere‚Äôs a 92% chance the student will **Pass**.‚Äù\n",
        "\n",
        "üéâ That‚Äôs **Forward Propagation** ‚Äî the model uses the input, weights, and bias to make a prediction.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Backward Propagation (How the network **learns**)\n",
        "\n",
        "Now suppose the **actual result** was:\n",
        "\n",
        "> The student **Failed** (i.e., actual = 0)\n",
        "\n",
        "So, your model predicted `0.92` but the correct answer was `0`.\n",
        "It made a big mistake!\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Calculate **Loss**\n",
        "\n",
        "Loss is like the penalty for being wrong.\n",
        "We use something like Mean Squared Error:\n",
        "\n",
        "```\n",
        "Loss = (Predicted - Actual)¬≤\n",
        "     = (0.92 - 0)¬≤ ‚âà 0.85\n",
        "```\n",
        "\n",
        "High loss means \"bad prediction.\"\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Backward Propagation = Fixing the Mistake\n",
        "\n",
        "Here‚Äôs what happens:\n",
        "\n",
        "1. The model figures out **which weights caused the mistake**.\n",
        "2. It adjusts each weight **a little** using gradients (how sensitive the loss is to each weight).\n",
        "3. This is done using something called **Gradient Descent** (a method to reduce the loss).\n",
        "4. Updated weights make the model better next time.\n",
        "\n",
        "### Example: Updating one weight\n",
        "\n",
        "Let‚Äôs say:\n",
        "\n",
        "* The weight for `hours_studied` caused more error than `sleep_hours`\n",
        "* So we **reduce** that weight from `0.4` ‚Üí `0.3`\n",
        "\n",
        "Next time, the prediction will move closer to the actual result.\n",
        "\n",
        "This is **learning**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Repeat\n",
        "\n",
        "Forward ‚Üí Predict\n",
        "Backward ‚Üí Fix mistakes\n",
        "Repeat for many students (many epochs), and the network gets really smart!\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary Table\n",
        "\n",
        "| Step      | Forward Propagation                | Backward Propagation                       |\n",
        "| --------- | ---------------------------------- | ------------------------------------------ |\n",
        "| Purpose   | Make a prediction                  | Learn from the mistake                     |\n",
        "| Data Flow | Input ‚Üí Output                     | Output ‚Üí Error ‚Üí Input (adjust weights)    |\n",
        "| Uses      | Weights, bias, activation function | Loss function, gradients, optimizer        |\n",
        "| Outcome   | A guess                            | Improved weights for better future guesses |\n",
        "\n"
      ],
      "metadata": {
        "id": "Y3iI_aetmv0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loss Function**\n",
        "\n",
        "\n",
        "\n",
        "## üí• What is a Loss Function?\n",
        "\n",
        "A **loss function** tells us **how wrong** the model‚Äôs prediction is.\n",
        "\n",
        "Think of it like this:\n",
        "\n",
        "> üß† ‚ÄúHey model, you guessed 92%, but the real answer was 0. That‚Äôs way off! Here‚Äôs how badly you messed up.‚Äù\n",
        "\n",
        "The loss function **quantifies the mistake**.\n",
        "\n",
        "---\n",
        "\n",
        "## üî¢ Simple Example\n",
        "\n",
        "Imagine this case:\n",
        "\n",
        "| Input (Hours Studied) | Actual Result | Model‚Äôs Prediction |\n",
        "| --------------------- | ------------- | ------------------ |\n",
        "| 2                     | 0 (Fail)      | 0.92               |\n",
        "\n",
        "We use a **loss function** to measure the difference between:\n",
        "\n",
        "* Actual = 0\n",
        "* Predicted = 0.92\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Common Loss Functions (Simplified)\n",
        "\n",
        "### 1. **Mean Squared Error (MSE)**\n",
        "\n",
        "Used in **regression** (predicting numbers)\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\text{predicted}_i - \\text{actual}_i)^2\n",
        "$$\n",
        "\n",
        "* $n$ = number of data points\n",
        "* $\\text{predicted}_i$ = the prediction for the i-th data point\n",
        "* $\\text{actual}_i$ = the actual true value for the i-th data point\n",
        "\n",
        "\n",
        "\n",
        "###  Example:\n",
        "\n",
        "Let's say we have **just one prediction**:\n",
        "\n",
        "| Predicted | Actual | Calculation          |\n",
        "| --------- | ------ | -------------------- |\n",
        "| 0.92      | 0      | (0.92 - 0)¬≤ = 0.8464 |\n",
        "\n",
        "Since there‚Äôs only **one data point**, $n = 1$\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{1} \\times (0.92 - 0)^2 = (0.92)^2 = 0.8464\n",
        "$$\n",
        "\n",
        "‚úÖ So the **error** for this prediction is **0.8464**.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Binary Cross Entropy**\n",
        "\n",
        "Used for **binary classification** (like Pass/Fail, Yes/No)\n",
        "\n",
        "```text\n",
        "Loss = - [ y * log(p) + (1 - y) * log(1 - p) ]\n",
        "```\n",
        "\n",
        "* `y` is actual (0 or 1)\n",
        "* `p` is predicted probability (like 0.92)\n",
        "\n",
        "#### Example:\n",
        "\n",
        "If actual = 0, and predicted = 0.92:\n",
        "\n",
        "```\n",
        "Loss = - [0 * log(0.92) + (1 - 0) * log(1 - 0.92)]\n",
        "     = - log(0.08) ‚âà 2.525\n",
        "```\n",
        "\n",
        "üî¥ Very high! Because the model was confident about a wrong answer.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Categorical Cross Entropy**\n",
        "\n",
        "Used when there are **more than 2 classes** (e.g., Dog, Cat, Bird)\n",
        "\n",
        "---\n",
        "\n",
        "## üß† In Simple Words:\n",
        "\n",
        "| Loss Function            | Use For                    | What it Measures                         |\n",
        "| ------------------------ | -------------------------- | ---------------------------------------- |\n",
        "| MSE                      | Regression                 | Squared difference between guess & truth |\n",
        "| Binary CrossEntropy      | Binary classification      | How confident and correct the guess was  |\n",
        "| Categorical CrossEntropy | Multi-class classification | Confidence over many options             |\n",
        "\n",
        "---\n",
        "\n",
        "## üìâ Why It‚Äôs Important\n",
        "\n",
        "The model uses the **loss** to:\n",
        "\n",
        "* Know **how bad** its predictions are\n",
        "* Use **backpropagation** to fix its weights\n",
        "* Improve predictions over time\n",
        "\n",
        "> Loss is like a teacher giving a grade ‚Äî the model learns from it.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IuywbUvGqNoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Activation Function**\n",
        "\n",
        "\n",
        "## üß† What is an Activation Function?\n",
        "\n",
        "An **activation function** decides **whether a neuron should \"fire\"** (pass info forward) or not ‚Äî kind of like a filter for deciding what's important.\n",
        "\n",
        "> Without it, a neural network would just be a boring linear equation. Activation adds **learning power** and **complexity**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîå Simple Analogy:\n",
        "\n",
        "Think of a **light switch**:\n",
        "\n",
        "* If the signal is strong enough ‚Üí turn ON\n",
        "* If weak ‚Üí stay OFF\n",
        "\n",
        "An activation function acts **like a smart switch** inside each neuron.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Why is it needed?\n",
        "\n",
        "* It adds **non-linearity** ‚Äî lets the network learn complex patterns.\n",
        "* Helps the model learn things like images, voices, language, etc.\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Common Activation Functions (with Simple Examples)\n",
        "\n",
        "### 1. **ReLU (Rectified Linear Unit)**\n",
        "\n",
        "```python\n",
        "f(x) = max(0, x)\n",
        "```\n",
        "\n",
        "* If input is positive ‚Üí keep it\n",
        "* If input is negative ‚Üí set it to 0\n",
        "\n",
        "#### Example:\n",
        "\n",
        "```\n",
        "f(5) = 5  \n",
        "f(-3) = 0\n",
        "```\n",
        "\n",
        "‚úÖ Very popular because it‚Äôs fast and works well.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Sigmoid**\n",
        "\n",
        "```python\n",
        "f(x) = 1 / (1 + e^(-x))\n",
        "```\n",
        "\n",
        "* Squashes output between **0 and 1**\n",
        "* Good for binary classification\n",
        "\n",
        "#### Example:\n",
        "\n",
        "```\n",
        "f(3) ‚âà 0.95 (high confidence)\n",
        "f(-3) ‚âà 0.05 (low confidence)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Tanh (Hyperbolic Tangent)**\n",
        "\n",
        "```python\n",
        "f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
        "```\n",
        "\n",
        "* Output is between **-1 and 1**\n",
        "* Better than sigmoid in many cases\n",
        "\n",
        "#### Example:\n",
        "\n",
        "```\n",
        "f(2) ‚âà 0.96  \n",
        "f(-2) ‚âà -0.96\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Quick Summary Table\n",
        "\n",
        "| Activation | Output Range | Use Case                                      | Example Input | Output |\n",
        "| ---------- | ------------ | --------------------------------------------- | ------------- | ------ |\n",
        "| ReLU       | 0 to ‚àû       | Hidden layers (general)                       | -3            | 0      |\n",
        "| Sigmoid    | 0 to 1       | Binary output layer                           | 3             | 0.95   |\n",
        "| Tanh       | -1 to 1      | Hidden layers (sometimes better than sigmoid) | -2            | -0.96  |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† In Real Life:\n",
        "\n",
        "Let‚Äôs say:\n",
        "\n",
        "* Your neuron calculates a raw score of `-3`\n",
        "* Without activation ‚Üí passes `-3` forward\n",
        "* With ReLU ‚Üí passes `0` (ignores it)\n",
        "* With Sigmoid ‚Üí passes `~0.05` (low confidence)\n",
        "* With Tanh ‚Üí passes `~ -0.96` (strong negative signal)\n",
        "\n",
        "So the activation controls **how much signal is passed on.**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## üß† When to Use Which Activation Function (Simple Guide)\n",
        "\n",
        "| Activation Function | When to Use It                                       | Why                                                                       |\n",
        "| ------------------- | ---------------------------------------------------- | ------------------------------------------------------------------------- |\n",
        "| **ReLU**            | Hidden layers of deep networks                       | Fast, simple, and works really well. Stops negative values. Most popular! |\n",
        "| **Leaky ReLU**      | If ReLU is giving 0s too much (dead neurons problem) | Like ReLU, but keeps small negative values (helps learning continue)      |\n",
        "| **Sigmoid**         | Output layer for **binary classification** (0 or 1)  | Converts to probability (between 0 and 1)                                 |\n",
        "| **Tanh**            | Hidden layers if your data has negative values       | Better than sigmoid because it outputs between -1 and 1                   |\n",
        "| **Softmax**         | Output layer for **multi-class classification**      | Turns output into probabilities that add up to 1 (e.g., dog/cat/bird)     |\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Examples to Remember\n",
        "\n",
        "### ‚úÖ Use ReLU in hidden layers:\n",
        "\n",
        "```text\n",
        "For almost any deep neural network.\n",
        "```\n",
        "\n",
        "### ‚úÖ Use Sigmoid at the output:\n",
        "\n",
        "```text\n",
        "If you're predicting:\n",
        "- Spam or not spam\n",
        "- Cancer or no cancer\n",
        "- Pass or fail\n",
        "```\n",
        "\n",
        "### ‚úÖ Use Softmax at the output:\n",
        "\n",
        "```text\n",
        "If you're classifying:\n",
        "- Dog, Cat, or Bird\n",
        "- Apple, Banana, or Orange\n",
        "```\n",
        "\n",
        "### ‚úÖ Use Tanh in hidden layers (optional):\n",
        "\n",
        "```text\n",
        "If your data is centered around zero (e.g., -1 to 1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ One-Line Summary\n",
        "\n",
        "> ‚úÖ Use **ReLU** in hidden layers,\n",
        "> ‚úÖ Use **Sigmoid** for binary output,\n",
        "> ‚úÖ Use **Softmax** for multi-class output.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jjwTZvqwt_00"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pje1vyOnmGyp"
      },
      "outputs": [],
      "source": []
    }
  ]
}